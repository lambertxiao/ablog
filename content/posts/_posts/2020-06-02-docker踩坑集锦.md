---
layout: post
title: "docker踩坑集锦"
date: 2020-06-02
tags: [k8s, docker]
comments: false
---


## 踩坑集锦

此篇文章用来记录维护k8s过程中的一系列问题

---

## 容器端口被占用

```
Bind for 0.0.0.0:9876 failed: port is already allocated.
```

1. 找到端口占用的进程

  ```
  sudo lsof -i -P -n | grep 9000
  ```

2. 杀掉进程

3. 重启docker

---

### kubectl logs 超时

错误类似：

```
Error from server: Get https://172.31.27.3:10250/containerLogs/prod-sfox/a-group-miner-2hshz/miner-idx-1?follow=true&tailLines=20: dial tcp 172.31.27.3:10250: connect: connection timed out
```

* 该操作走内网ip，节点间内网不同

* 10250端口未打开

---

### DNS解析失败

具体错误信息一般如下：

```
panic: dial tcp: lookup mysql on 10.96.0.10:53: no such host
```

* 查看coredns的日志，判断集群内的dns解析是否成功

* 如果集群内成功，是往外网的dns解析出错，检查宿主机的 `/etc/resolv.conf ` 配置

---

### Helm报错

```
UPGRADE FAILED
Error: "dev-chain" has no deployed releases
Error: UPGRADE FAILED: "dev-chain" has no deployed releases
```

原因是helm的bug，如果helm有一次deploy的失败记录就没法重新deploy, 需要手动通过 `helm delete` 命令删除错误的记录

---

### 容器组里的服务获取不到客户端真实IP

如果你的服务暴露方式是NodePort，当客户端访问IP所属的Node和服务所在的Node不是同一个Node时，因为内部有SNAT和DNAT的存在，
客户端的源IP会丢失，解决方法，在Service的配置中添加 `externalTrafficPolicy`, 并用 `nodeSelector` 将pod固定在nodePort对应的同一台机器上

```yaml
apiVersion: v1
kind: Service
metadata:
  namespace: dev
  name: signal
spec:
  externalTrafficPolicy: Local
  type: NodePort
  ports:
    - port: 8080
      targetPort: 8080
      name: http
      nodePort: 30010
  selector:
    app: signal
```

---

### docker启动容器报错

```
docker: Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused "process_linux.go:301: running exec setns process for init caused “exit status 40"”: unknown.
```

解决方法:

```
echo 1 > /proc/sys/vm/drop_caches
echo vm.min_free_kbytes=1048576 >> /etc/sysctl.conf
sysctl -p
```

---

### 公用解决方法

* 查看kubelet日志

    ```
    systemctl status kubelet
    journalctl -xu kubelet
    ```

* 查看docker的日志

    ```
    systemctl status docker
    journalctl -xu docker
    ```

### starting metrics server failed: listen tcp 127.0.0.1:10249: bind: address already in use

重启kube-proxy
